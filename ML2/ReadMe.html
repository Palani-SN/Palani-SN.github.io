<!DOCTYPE html>
<html>

<head>
  <title>README.md</title>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">

  <style>
    /* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
    /*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

    body {
      font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
      font-size: var(--vscode-markdown-font-size, 14px);
      padding: 0 26px;
      line-height: var(--vscode-markdown-line-height, 22px);
      word-wrap: break-word;
    }

    #code-csp-warning {
      position: fixed;
      top: 0;
      right: 0;
      color: white;
      margin: 16px;
      text-align: center;
      font-size: 12px;
      font-family: sans-serif;
      background-color: #444444;
      cursor: pointer;
      padding: 6px;
      box-shadow: 1px 1px 1px rgba(0, 0, 0, .25);
    }

    #code-csp-warning:hover {
      text-decoration: none;
      background-color: #007acc;
      box-shadow: 2px 2px 2px rgba(0, 0, 0, .25);
    }

    body.scrollBeyondLastLine {
      margin-bottom: calc(100vh - 22px);
    }

    body.showEditorSelection .code-line {
      position: relative;
    }

    body.showEditorSelection .code-active-line:before,
    body.showEditorSelection .code-line:hover:before {
      content: "";
      display: block;
      position: absolute;
      top: 0;
      left: -12px;
      height: 100%;
    }

    body.showEditorSelection li.code-active-line:before,
    body.showEditorSelection li.code-line:hover:before {
      left: -30px;
    }

    .vscode-light.showEditorSelection .code-active-line:before {
      border-left: 3px solid rgba(0, 0, 0, 0.15);
    }

    .vscode-light.showEditorSelection .code-line:hover:before {
      border-left: 3px solid rgba(0, 0, 0, 0.40);
    }

    .vscode-light.showEditorSelection .code-line .code-line:hover:before {
      border-left: none;
    }

    .vscode-dark.showEditorSelection .code-active-line:before {
      border-left: 3px solid rgba(255, 255, 255, 0.4);
    }

    .vscode-dark.showEditorSelection .code-line:hover:before {
      border-left: 3px solid rgba(255, 255, 255, 0.60);
    }

    .vscode-dark.showEditorSelection .code-line .code-line:hover:before {
      border-left: none;
    }

    .vscode-high-contrast.showEditorSelection .code-active-line:before {
      border-left: 3px solid rgba(255, 160, 0, 0.7);
    }

    .vscode-high-contrast.showEditorSelection .code-line:hover:before {
      border-left: 3px solid rgba(255, 160, 0, 1);
    }

    .vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
      border-left: none;
    }

    img {
      max-width: 100%;
      max-height: 100%;
    }

    a {
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    a:focus,
    input:focus,
    select:focus,
    textarea:focus {
      outline: 1px solid -webkit-focus-ring-color;
      outline-offset: -1px;
    }

    hr {
      border: 0;
      height: 2px;
      border-bottom: 2px solid;
    }

    h1 {
      padding-bottom: 0.3em;
      line-height: 1.2;
      border-bottom-width: 1px;
      border-bottom-style: solid;
    }

    h1,
    h2,
    h3 {
      font-weight: normal;
    }

    h4,
    h5,
    h6 {
      font-size: small;
      margin: 1%;
    }

    table {
      border-collapse: collapse;
    }

    table>thead>tr>th {
      text-align: left;
      border-bottom: 1px solid;
    }

    table>thead>tr>th,
    table>thead>tr>td,
    table>tbody>tr>th,
    table>tbody>tr>td {
      padding: 5px 10px;
    }

    table>tbody>tr+tr>td {
      border-top: 1px solid;
    }

    blockquote {
      margin: 0 7px 0 5px;
      padding: 0 16px 0 10px;
      border-left-width: 5px;
      border-left-style: solid;
    }

    code {
      font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
      font-size: 1em;
      line-height: 1.357em;
    }

    body.wordWrap pre {
      white-space: pre-wrap;
    }

    pre:not(.hljs),
    pre.hljs code>div {
      padding: 16px;
      border-radius: 3px;
      overflow: auto;
    }

    pre code {
      color: var(--vscode-editor-foreground);
      tab-size: 4;
    }

    /** Theming */

    .vscode-light pre {
      background-color: rgba(220, 220, 220, 0.4);
    }

    .vscode-dark pre {
      background-color: rgba(10, 10, 10, 0.4);
    }

    .vscode-high-contrast pre {
      background-color: rgb(0, 0, 0);
    }

    .vscode-high-contrast h1 {
      border-color: rgb(0, 0, 0);
    }

    .vscode-light table>thead>tr>th {
      border-color: rgba(0, 0, 0, 0.69);
    }

    .vscode-dark table>thead>tr>th {
      border-color: rgba(255, 255, 255, 0.69);
    }

    .vscode-light h1,
    .vscode-light hr,
    .vscode-light table>tbody>tr+tr>td {
      border-color: rgba(0, 0, 0, 0.18);
    }

    .vscode-dark h1,
    .vscode-dark hr,
    .vscode-dark table>tbody>tr+tr>td {
      border-color: rgba(255, 255, 255, 0.18);
    }
  </style>

  <style>
    /* Tomorrow Theme */
    /* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
    /* Original theme - https://github.com/chriskempson/tomorrow-theme */

    /* Tomorrow Comment */
    .hljs-comment,
    .hljs-quote {
      color: #8e908c;
    }

    /* Tomorrow Red */
    .hljs-variable,
    .hljs-template-variable,
    .hljs-tag,
    .hljs-name,
    .hljs-selector-id,
    .hljs-selector-class,
    .hljs-regexp,
    .hljs-deletion {
      color: #c82829;
    }

    /* Tomorrow Orange */
    .hljs-number,
    .hljs-built_in,
    .hljs-builtin-name,
    .hljs-literal,
    .hljs-type,
    .hljs-params,
    .hljs-meta,
    .hljs-link {
      color: #f5871f;
    }

    /* Tomorrow Yellow */
    .hljs-attribute {
      color: #eab700;
    }

    /* Tomorrow Green */
    .hljs-string,
    .hljs-symbol,
    .hljs-bullet,
    .hljs-addition {
      color: #718c00;
    }

    /* Tomorrow Blue */
    .hljs-title,
    .hljs-section {
      color: #4271ae;
    }

    /* Tomorrow Purple */
    .hljs-keyword,
    .hljs-selector-tag {
      color: #8959a8;
    }

    .hljs {
      display: block;
      overflow-x: auto;
      color: #4d4d4c;
      padding: 0.5em;
    }

    .hljs-emphasis {
      font-style: italic;
    }

    .hljs-strong {
      font-weight: bold;
    }
  </style>

  <style>
    /*
 * Markdown PDF CSS
 */

    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
      padding: 0 12px;
    }

    pre {
      background-color: #f8f8f8;
      border: 1px solid #cccccc;
      border-radius: 3px;
      overflow-x: auto;
      white-space: pre-wrap;
      overflow-wrap: break-word;
    }

    pre:not(.hljs) {
      padding: 23px;
      line-height: 19px;
    }

    blockquote {
      background: rgba(127, 127, 127, 0.1);
      border-color: rgba(0, 122, 204, 0.5);
    }

    .emoji {
      height: 1.4em;
    }

    code {
      font-size: 14px;
      line-height: 19px;
    }

    /* for inline code */
    :not(pre):not(.hljs)>code {
      color: #C9AE75;
      /* Change the old color so it seems less like an error */
      font-size: inherit;
    }

    /* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
    .page {
      page-break-after: always;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>

<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
        ? 'dark'
        : 'default'
    });
  </script>
  <h2 id="mechanistic-interpretability-decomposing-latent-logic-via-sparse-autoencoders">Mechanistic Interpretability:
    Decomposing Latent Logic via Sparse Autoencoders</h2>
  <blockquote>
    <p>We demonstrate that Sparse Autoencoders can decompose a neural network's internal logic into causal, steerable
      features, achieving up to 92% control on out-of-distribution inputs while revealing fundamental trade-offs between
      different steering objectives.</p>
  </blockquote>
  <h3 id="introduction">Introduction</h3>
  <ul>
    <li>This study investigates the causal structure of a neural network trained on multi-objective logical regression.
      By utilizing <strong>Sparse Autoencoders (SAE)</strong>, we successfully decompose polysemantic activations into
      monosemantic latent features. We demonstrate that high-level abstract concepts—specifically <strong>Sign</strong>
      and <strong>Subset</strong>—are represented as linear directions in latent space. Through <strong>Activation
        Steering</strong> and <strong>Surgical Ablation</strong>, we prove the causality of these features, achieving
      strong control in specific regimes across Out-of-Distribution (OOD) datasets.</li>
  </ul>
  <h3 id="problem-statement-index-based-arithmetic-routing">Problem Statement: Index-Based Arithmetic Routing</h3>
  <p>To evaluate the interpretive capabilities of Sparse Autoencoders (SAEs), we designed a non-trivial
    <strong>Index-Based Arithmetic</strong> task. Unlike standard regression, this task requires a Multi-Layer
    Perceptron (MLP) to perform conditional &quot;routing&quot; logic before executing an arithmetic operation.
  </p>
  <h3 id="key-findings">Key Findings</h3>
  <p>Having established this challenging task, our experiments revealed:</p>
  <ol>
    <li><strong>Geometric Disentanglement</strong>: Sign and magnitude features are orthogonal (cosine similarity =
      -0.02), enabling independent control</li>
    <li><strong>OOD Superiority Paradox</strong>: Steering succeeds more on extrapolation (92.6%) than interpolation
      (98.1%)</li>
    <li><strong>Perfect Scaling Control</strong>: 100% accuracy on magnitude-shifted inputs validates linear
      representation</li>
    <li><strong>Steering Stiffness Hierarchy</strong>: Sign requires 100× more α than magnitude, revealing fundamental
      controllability limits</li>
  </ol>
  <h4 id="task-mechanics">Task Mechanics</h4>
  <p>The model is presented with a 5 x 2 matrix (flattened into a 10-element vector). The network must learn a
    two-stage logic process:</p>
  <ol>
    <li><strong>Pointer Identification (Routing):</strong> The final element of each column serves as a dynamic
      <em><strong>pointer (index)</strong></em>. The model must look at these pointers to identify which values in the
      preceding rows are relevant.
    </li>
    <li><strong>Arithmetic Execution:</strong> Once the values are &quot;fetched&quot; based on the pointers, the model
      must calculate the signed difference between the two selected numbers.</li>
  </ol>

  <p>
    $$Target = \text{Value}_{1}[\text{Index}_{1}] - \text{Value}_{2}[\text{Index}_{2}]$$
  </p>

  <h4 id="why-this-task">Why this Task?</h4>
  <ul>
    <li><strong>Conditional Logic:</strong> By making the target value dependent on the <em>position</em> indicated by a
      pointer, we force the MLP to develop internal &quot;switching&quot; circuits. This is significantly more complex
      than simple linear regression.</li>
    <li><strong>Disentanglement Testing:</strong> The generator creates balanced datasets across four distinct
      &quot;concept quadrants&quot;.</li>
    <li><strong>Sign Control:</strong> Positive vs. Negative results.</li>
    <li><strong>Subset Control:</strong> &quot;Small&quot; (≤ 5) vs. &quot;Large&quot; (> 5) absolute values.
    </li>
    <li><strong>Mechanistic Mapping:</strong> The primary objective is to determine if an SAE can isolate specific
      features (latents) that correspond to these hidden &quot;pointer&quot; operations and the resulting conceptual
      logic (sign and subset).</li>
  </ul>
  <h4 id="dataset-variations">Dataset Variations</h4>
  <p>The generator produces several specialized splits to test the model's robustness and generalization:</p>
  <ul>
    <li><strong>Interpolation:</strong> Standard ranges (1 to 9) to test core logic.</li>
    <li><strong>Extrapolation:</strong> Out-of-distribution integer ranges (10 to 20).</li>
    <li><strong>Scaling:</strong> High-magnitude integer inputs (100 to 110).</li>
    <li><strong>Precision:</strong> Floating-point inputs to test numerical sensitivity.</li>
  </ul>
  <h3 id="folder-structure">Folder Structure</h3>
  <ul>
    <li><a href="https://github.com/Palani-SN/Activation-Steering-et-Ablation">Github</a></li>
  </ul>
  <pre class="hljs"><code><div>Activation-Steering-et-Ablation/
├── benchmarking.py               # Steering validation and compliance testing across OOD datasets
├── feature_probe.py              # Steering basis extraction, feature probing, ablation experiments
├── feature_reports.py            # Visualization suite: compass, heatmaps, logit-lens
├── harvest_activations.py        # Harvests activations from trained MLP for SAE training
├── README.md                     # Main experiment guide and walkthrough
├── reqs.txt                      # Python package requirements
├── train_mlp.py                  # MLP training script
├── train_sae.py                  # SAE training script
├── workflow.bat                  # Batch script for end-to-end pipeline execution
├── dataset/                      # Dataset generation and loading utilities
│   ├── data_generator.py         # Generates primary dataset with concept tags
│   ├── data_loader.py            # Loads datasets, provides concept mapping
│   ├── variant_generator.py      # Generates OOD dataset variants
│   └── __pycache__/              # Python cache files
├── images/                       # Output visualizations and experiment images
│   └── README.md                 # Image documentation
├── mlp/                          # MLP model definitions and weights
│   ├── mlp_definition.py         # Custom MLP architecture for interpretability
│   ├── perfect_mlp.pth           # Trained MLP weights
│   └── __pycache__/              # Python cache files
├── sae/                          # SAE model definitions and weights
│   ├── sae_definition.py         # Sparse Autoencoder architecture
│   ├── universal_sae.pth         # Trained SAE weights
│   └── __pycache__/              # Python cache files
├── temp/                         # Temporary outputs and intermediate data
│   ├── feature_subsets.pt        # Saved feature subsets for analysis
│   ├── harvested_data.pt         # Saved activations and metadata
│   ├── steering_basis.pt         # Saved steering basis vectors
│   └── alpha_sweep_results.pkl   # Steering performance heatmap data
</div></code></pre>
  <p>Each file and folder is annotated with its main purpose in the experiment pipeline. See below for detailed
    walkthroughs and code references.</p>
  <h3 id="env-setup">Env Setup</h3>
  <ul>
    <li>setup conda env, with <em><strong>python 3.11.6</strong></em></li>
  </ul>
  <pre class="hljs"><code><div>git clone https://github.com/Palani-SN/Activation-Steering-et-Ablation.git
<span class="hljs-built_in">cd</span> Activation-Steering-et-Ablation
conda create -n act-abl python=<span class="hljs-number">3</span>.<span class="hljs-number">11</span>.<span class="hljs-number">6</span>
conda activate act-abl
python -m pip install torch==<span class="hljs-number">2</span>.<span class="hljs-number">10</span>.<span class="hljs-number">0</span> torchvision==<span class="hljs-number">0</span>.<span class="hljs-number">25</span>.<span class="hljs-number">0</span> --extra-index-url https://download.pytorch.org/whl/cu126
python -m pip install -r reqs.txt
</div></code></pre>
  <h3 id="execution-steps--technical-details">Execution Steps &amp; Technical Details</h3>
  <ul>
    <li><strong>Dataset Generation (<code>data_generator.py</code>)</strong>: Generates matrices with custom logic
      categories, ensuring balanced representation across <strong>Sign</strong> (Positive/Negative) and
      <strong>Subset</strong> (0-5 vs. 5-10).
    </li>
    <li><strong>MLP Training (<code>train_mlp.py</code>)</strong>: Optimizes a network (typically with a 256 or 512-dim
      hidden layer) to achieve near-zero MSE on the index-based subtraction task.</li>
    <li><strong>Activation Harvesting (<code>harvest_activations.py</code>)</strong>: Hooks the MLP's
      <code>hidden2</code> layer to capture internal representations as <code>harvested_data.pt</code>.
    </li>
    <li><strong>Top-K SAE Learning (<code>train_sae.py</code>)</strong>: Trains a Sparse Autoencoder using a
      <strong>Top-K activation function</strong>. By enforcing a hard L_0 sparsity constraint (k=128), the SAE
      identifies the most potent features while avoiding the &quot;shrinkage&quot; common in L1-based models.
    </li>
    <li><strong>Feature Probing (<code>feature_probe.py</code>)</strong>: Isolates &quot;Specialist&quot; features that
      represent specific logical states, such as &quot;Positive Sign&quot; or &quot;Large Subset&quot;.</li>
    <li><strong>Causal Validation (<code>benchmarking.py</code>)</strong>: Uses <strong>Activation Steering</strong> to
      verify the role of identified features. By injecting feature-basis vectors into the latent space, we can manually
      &quot;force&quot; the model to flip its output (e.g., changing a predicted -10 to a +1).</li>
    <li><strong>Feature Reporting (<code>feature_reports.py</code>)</strong>: Visualizes the <strong>Concept
        Compass</strong> and <strong>Logit-Lens</strong>, providing high-fidelity maps of how specific SAE features
      drive the final model output.</li>
  </ul>
  <p><img src="images/workflow.png" alt="workflow">
    <em>Figure 1: Scripts, Inventories &amp; Visualization plots specified as per the order of Execution.</em>
  </p>
  <h3 id="execution-log">Execution Log</h3>
  <ul>
    <li>To check the complete Execution Log, <a
        href="https://github.com/Palani-SN/Activation-Steering-et-Ablation/blob/main/workflow.log">check here</a></li>
  </ul>
  <h3 id="execution-steps">Execution Steps</h3>
  <h4 id="dataset-creation">Dataset Creation</h4>
  <ul>
    <li>Creates All required Datasets, for Training &amp; OOD Benchmarking in the right proportion of Concept Classes.
    </li>
  </ul>
  <pre class="hljs"><code><div>[2/8] &gt; Generating Dataset...
Generating 8000 balanced rows for mlp_train.xlsx...
Successfully saved mlp_train.xlsx
Generating 1000 balanced rows for mlp_val.xlsx...
Successfully saved mlp_val.xlsx
Generating 1000 balanced rows for mlp_test.xlsx...
Successfully saved mlp_test.xlsx
Generating 1000 balanced rows for interp_test.xlsx...
Successfully saved interp_test.xlsx
Generating 1000 balanced rows for extrap_test.xlsx...
Successfully saved extrap_test.xlsx
Generating 1000 balanced rows for scaling_test.xlsx...
Successfully saved scaling_test.xlsx
Generating 1000 balanced rows for precision_test.xlsx...
Successfully saved precision_test.xlsx
[OK] All datasets generated
</div></code></pre>
  <h4 id="train-mlp">Train MLP</h4>
  <pre class="hljs"><code><div>[3/8] &gt; Training MLP...

================================================================================
  PHASE I: TRAINING MLP TO INTERPRETABLE PERFECTION
================================================================================
  Device: cuda
  Total Epochs: 500
  Batch Size: 256
================================================================================

  [███░░░░░░░░░░░░░░░░░░░░░░░░░░░] Epoch  50/500 | Val MSE: 0.359431 |  10.0%
  [██████░░░░░░░░░░░░░░░░░░░░░░░░] Epoch 100/500 | Val MSE: 0.306344 |  20.0%
  [█████████░░░░░░░░░░░░░░░░░░░░░] Epoch 150/500 | Val MSE: 0.167713 |  30.0%
  [████████████░░░░░░░░░░░░░░░░░░] Epoch 200/500 | Val MSE: 0.245994 |  40.0%
  [███████████████░░░░░░░░░░░░░░░] Epoch 250/500 | Val MSE: 0.234685 |  50.0%
  [██████████████████░░░░░░░░░░░░] Epoch 300/500 | Val MSE: 0.128852 |  60.0%
  [█████████████████████░░░░░░░░░] Epoch 350/500 | Val MSE: 0.068327 |  70.0%
  [████████████████████████░░░░░░] Epoch 400/500 | Val MSE: 0.045738 |  80.0%
  [███████████████████████████░░░] Epoch 450/500 | Val MSE: 0.039459 |  90.0%
  [██████████████████████████████] Epoch 500/500 | Val MSE: 0.036946 | 100.0%

================================================================================
  FINAL PERFORMANCE ANALYSIS
================================================================================

  Per-Concept Metrics:
  ------------------------------------------------------------------
  → +00 &lt; Pos &lt;= +05     | MSE: 0.038132 | Samples:  250
  → +05 &lt; Pos &lt;= +10     | MSE: 0.031488 | Samples:  250
  → -05 &lt;= Neg &lt; +00     | MSE: 0.031514 | Samples:  250
  → -10 &lt;= Neg &lt; -05     | MSE: 0.037543 | Samples:  250
  ------------------------------------------------------------------
  ✓ Total Test MSE: 0.034669
================================================================================

[OK] MLP trained to perfection
</div></code></pre>
  <ul>
    <li>We use dataset of 8k samples to train the mlp and save it to .pth file for later analysis.</li>
  </ul>
  <p><img src="images/mlp_training.png" alt="mlp_training"></p>
  <p><em>Figure 2: MLP Trained to an Accuracy of MSE 0.034669.</em></p>
  <h4 id="harvest-activations">Harvest Activations</h4>
  <ul>
    <li>We have collected the decomposed features from 256 neurons at layer 'hidden2' from mlp, for all 8k samples of
      training set.</li>
  </ul>
  <pre class="hljs"><code><div>[4/8] &gt; Harvesting Activations...

================================================================================
  HARVESTING ACTIVATIONS FROM TRAINED MLP
================================================================================
  Device: cuda
  Expected Samples: ~8000
================================================================================

  -&gt; Harvesting activations on cuda...

  [OK] Successfully saved 8000 activations with metadata.
================================================================================

[OK] Activations harvested
</div></code></pre>
  <h4 id="train-sae">Train SAE</h4>
  <pre class="hljs"><code><div>[5/8] &gt; Training Sparse Autoencoder (SAE)...

================================================================================
  PHASE II: TRAINING SPARSE AUTOENCODER (SAE)
================================================================================
  Input Dimension: 256
  Dictionary Size: 2048
  Sparsity (k): 128
  Total Epochs: 100
  Batch Size: 128
================================================================================

  [███░░░░░░░░░░░░░░░░░░░░░░░░░░░] Epoch  10/100 | MSE: 0.098243 |  10.0%
  [██████░░░░░░░░░░░░░░░░░░░░░░░░] Epoch  20/100 | MSE: 0.038129 |  20.0%
  [█████████░░░░░░░░░░░░░░░░░░░░░] Epoch  30/100 | MSE: 0.022968 |  30.0%
  [████████████░░░░░░░░░░░░░░░░░░] Epoch  40/100 | MSE: 0.017078 |  40.0%
  [███████████████░░░░░░░░░░░░░░░] Epoch  50/100 | MSE: 0.013290 |  50.0%
  [██████████████████░░░░░░░░░░░░] Epoch  60/100 | MSE: 0.011450 |  60.0%
  [█████████████████████░░░░░░░░░] Epoch  70/100 | MSE: 0.010191 |  70.0%
  [████████████████████████░░░░░░] Epoch  80/100 | MSE: 0.008907 |  80.0%
  [███████████████████████████░░░] Epoch  90/100 | MSE: 0.008017 |  90.0%
  [██████████████████████████████] Epoch 100/100 | MSE: 0.006879 | 100.0%

================================================================================
  [OK] SAE Training Complete!
================================================================================

[OK] SAE trained successfully
</div></code></pre>
  <ul>
    <li>We will be training the SAE with the harvested Activations, to map the 256 neurons to top k (128+) features out
      of 2048 available sae features.</li>
  </ul>
  <p><img src="images/sae_training.png" alt="sae_training"></p>
  <p><em>Figure 3: SAE Trained to an Accuracy of MSE 0.006879.</em></p>
  <h4 id="feature-probe">Feature Probe</h4>
  <h5 id="orthogonality-check-through-cosine-similarity">Orthogonality Check through Cosine Similarity</h5>
  <ul>
    <li>As first step, we will be Calculating latent vectors, for both sign &amp; subset, that eventually pose as the
      base foundations for the concept as per problem statement.</li>
  </ul>
  <pre class="hljs"><code><div>=====================================================================================
  STEERING BASIS VECTORS ANALYSIS
=====================================================================================
  Sign-subset Cosine Similarity: -0.0404
  Interpretation: Near 0.0 → concepts are perfectly disentangled ✓
=====================================================================================
</div></code></pre>
  <h5 id="primitive-alpha-sweep-compliance-against-pca">Primitive Alpha Sweep Compliance Against PCA</h5>
  <ul>
    <li>We internally do calibration to normalize the vectors against each other based on mu &amp; sigma, then use it to
      verify the Alpha sweep compliance against PCA, as shown below.</li>
  </ul>
  <pre class="hljs"><code><div>=====================================================================================
  COMPLIANCE EVALUATION: SAE vs PCA Steering
=====================================================================================
[Calibration] Subset steering scaled by 2.978 to match sign effect.

Alpha Sweep Compliance (SAE vs PCA):
  Alpha=  0.5 | SAE Sign: 32/100 (32.0%) | PCA Sign: 20/100 (20.0%) | SAE Subset: 81/100 (81.0%) | PCA Subset: 55/100 (55.0%)
  Alpha=  1.0 | SAE Sign: 41/100 (41.0%) | PCA Sign: 20/100 (20.0%) | SAE Subset: 85/100 (85.0%) | PCA Subset: 54/100 (54.0%)
  Alpha=  2.0 | SAE Sign: 81/100 (81.0%) | PCA Sign: 20/100 (20.0%) | SAE Subset: 79/100 (79.0%) | PCA Subset: 54/100 (54.0%)
  Alpha=  4.0 | SAE Sign: 100/100 (100.0%) | PCA Sign: 20/100 (20.0%) | SAE Subset: 99/100 (99.0%) | PCA Subset: 55/100 (55.0%)
  Alpha=  8.0 | SAE Sign: 100/100 (100.0%) | PCA Sign: 20/100 (20.0%) | SAE Subset: 98/100 (98.0%) | PCA Subset: 56/100 (56.0%)
  Alpha= 16.0 | SAE Sign: 100/100 (100.0%) | PCA Sign: 19/100 (19.0%) | SAE Subset: 98/100 (98.0%) | PCA Subset: 52/100 (52.0%)
  Alpha= 32.0 | SAE Sign: 100/100 (100.0%) | PCA Sign: 19/100 (19.0%) | SAE Subset: 79/100 (79.0%) | PCA Subset: 55/100 (55.0%)
  Alpha= 64.0 | SAE Sign: 100/100 (100.0%) | PCA Sign: 17/100 (17.0%) | SAE Subset: 44/100 (44.0%) | PCA Subset: 59/100 (59.0%)

=====================================================================================
</div></code></pre>
  <ul>
    <li>this certainly proves that the sae model contains ground truth, in the form of internal wiring, as it performs
      better than generic PCA Accuracy.</li>
  </ul>
  <h5 id="top-k-features-per-concept-group">Top K Features Per Concept Group</h5>
  <ul>
    <li>We calculate top k features per concept group and use boolean Masking / Indexing to find common features that
      indicates monosemanticity of the Trained MLP model.</li>
  </ul>
  <pre class="hljs"><code><div>=====================================================================================
  TOP-128 FEATURES PER CONCEPT GROUP
=====================================================================================
  -10 &lt;= neg &lt; -05             : [1354, 803, 2043, 1362, 1550, 1347, 470, 1275, 1615, 1297, 1782, 940, 1281, 856, 1915, 1861, 1040, 612, 174, 1320, 1862, 1938, 2034, 175, 890, 559, 1896, 1477, 689, 865, 1113, 4, 432, 647, 896, 212, 285, 677, 1223, 256, 1867, 686, 1006, 1279, 835, 1608, 1450, 714, 582, 862, 1633, 1173, 1789, 55, 153, 36, 355, 1489, 1391, 979, 301, 1659, 1057, 215, 1373, 405, 798, 1100, 188, 850, 1509, 308, 590, 2035, 621, 1775, 323, 1384, 1366, 597, 608, 1389, 1562, 1894, 1496, 1788, 172, 1453, 312, 687, 48, 1322, 13, 2036, 1842, 1848, 1813, 1285, 1137, 1501, 1545, 415, 527, 1723, 490, 1011, 2022, 639, 1323, 497, 1543, 1794, 357, 1908, 1648, 1950, 474, 1863, 1399, 211, 1265, 1517, 1725, 1368, 616, 872, 423, 1146]
  +00 &lt; pos &lt;= +05             : [1320, 1347, 2043, 4, 865, 175, 1861, 1354, 803, 1862, 612, 1550, 1040, 470, 890, 1275, 1915, 1896, 1223, 1782, 432, 1281, 2034, 1362, 1113, 1006, 856, 940, 174, 1615, 686, 714, 212, 647, 896, 1938, 677, 1279, 256, 1297, 1477, 559, 1867, 689, 1450, 582, 285, 835, 1608, 1633, 1789, 1173, 1489, 862, 850, 153, 55, 36, 1391, 355, 1373, 308, 215, 1100, 1659, 301, 590, 1057, 1509, 798, 1775, 2035, 188, 172, 323, 621, 405, 1366, 48, 597, 2036, 1894, 979, 687, 527, 1322, 415, 1545, 1389, 1501, 1562, 1058, 1517, 608, 1137, 1813, 1723, 1384, 1842, 1788, 1848, 1146, 2022, 312, 1496, 1122, 1011, 490, 1265, 1950, 669, 534, 1690, 357, 1794, 639, 1543, 1241, 1863, 211, 1453, 1285, 1368, 872, 474, 1802, 497, 430]
  +05 &lt; pos &lt;= +10             : [865, 1320, 1347, 4, 890, 1862, 175, 1861, 2043, 1915, 612, 1275, 1040, 470, 432, 1354, 1896, 803, 1006, 1223, 1782, 1281, 1113, 686, 2034, 1550, 714, 940, 1615, 1279, 896, 212, 856, 1362, 174, 677, 647, 256, 1938, 1450, 582, 1867, 689, 1608, 1477, 1633, 559, 285, 862, 850, 1297, 1489, 36, 1173, 835, 1391, 1789, 1100, 153, 308, 590, 1659, 355, 215, 55, 1373, 798, 301, 2036, 48, 1775, 1509, 527, 1058, 1366, 415, 323, 1894, 1057, 405, 621, 687, 188, 172, 597, 1723, 1146, 1389, 1788, 2035, 1517, 490, 979, 1794, 1501, 639, 1562, 1690, 608, 1842, 474, 1241, 430, 534, 1545, 1322, 1137, 1950, 200, 312, 1011, 1813, 1848, 1863, 1734, 669, 211, 2022, 1122, 1802, 984, 1543, 173, 357, 423, 1368, 1399, 1414]
  -05 &lt;= neg &lt; +00             : [1354, 803, 1347, 2043, 1550, 1320, 470, 1362, 1275, 1861, 1862, 612, 175, 865, 1915, 940, 1040, 1782, 4, 856, 1615, 1297, 890, 1281, 174, 432, 1113, 1896, 1938, 1223, 2034, 559, 1006, 686, 647, 1477, 896, 689, 677, 212, 1279, 285, 256, 714, 1867, 1608, 1450, 582, 1173, 1633, 835, 1789, 862, 1489, 55, 153, 36, 355, 1391, 301, 308, 1659, 1509, 215, 1100, 850, 1373, 1057, 590, 798, 405, 979, 188, 2035, 621, 1894, 323, 1775, 1366, 608, 172, 48, 1322, 1384, 1562, 1788, 597, 687, 2036, 415, 1723, 312, 1389, 1813, 527, 1137, 1501, 1545, 1842, 1848, 1496, 1950, 1517, 1285, 357, 669, 490, 1794, 1453, 1863, 2022, 423, 872, 1399, 534, 1011, 211, 1368, 639, 13, 1690, 1543, 1802, 1122, 1908, 497, 1146, 1058]

  [OK] Universal Common Features: [4, 36, 48, 55, 153, 172, 174, 175, 188, 211, 212, 215, 256, 285, 301, 308, 312, 323, 355, 357, 405, 415, 423, 432, 470, 474, 490, 497, 527, 534, 559, 582, 590, 597, 608, 612, 621, 639, 647, 669, 677, 686, 687, 689, 714, 798, 803, 835, 850, 856, 862, 865, 872, 890, 896, 940, 979, 1006, 1011, 1040, 1057, 1058, 1100, 1113, 1122, 1137, 1146, 1173, 1223, 1265, 1275, 1279, 1281, 1285, 1297, 1320, 1322, 1347, 1354, 1362, 1366, 1368, 1373, 1384, 1389, 1391, 1399, 1450, 1453, 1477, 1489, 1496, 1501, 1509, 1517, 1543, 1545, 1550, 1562, 1608, 1615, 1633, 1659, 1690, 1723, 1775, 1782, 1788, 1789, 1794, 1802, 1813, 1842, 1848, 1861, 1862, 1863, 1867, 1894, 1896, 1915, 1938, 1950, 2022, 2034, 2035, 2036, 2043]

=====================================================================================
</div></code></pre>
  <h5 id="derivatives-to-find-distinct-features">Derivatives to find Distinct Features</h5>
  <ul>
    <li>We calculate top k distinct features, which essentially is <strong>set(All Features per group) - set(Universal
        Common Features)</strong>, to derive few distinct features from every group.</li>
  </ul>
  <pre class="hljs"><code><div>=====================================================================================
  IDENTIFIED FEATURE SUBSETS (UNIONS)
=====================================================================================
  Positive Sign Features : [4, 36, 48, 55, 153, 172, 173, 174, 175, 188, 200, 211, 212, 215, 256, 285, 301, 308, 312, 323, 355, 357, 405, 415, 423, 430, 432, 470, 474, 490, 497, 527, 534, 559, 582, 590, 597, 608, 612, 621, 639, 647, 669, 677, 686, 687, 689, 714, 798, 803, 835, 850, 856, 862, 865, 872, 890, 896, 940, 979, 984, 1006, 1011, 1040, 1057, 1058, 1100, 1113, 1122, 1137, 1146, 1173, 1223, 1241, 1265, 1275, 1279, 1281, 1285, 1297, 1320, 1322, 1347, 1354, 1362, 1366, 1368, 1373, 1384, 1389, 1391, 1399, 1414, 1450, 1453, 1477, 1489, 1496, 1501, 1509, 1517, 1543, 1545, 1550, 1562, 1608, 1615, 1633, 1659, 1690, 1723, 1734, 1775, 1782, 1788, 1789, 1794, 1802, 1813, 1842, 1848, 1861, 1862, 1863, 1867, 1894, 1896, 1915, 1938, 1950, 2022, 2034, 2035, 2036, 2043]
  Subset 0-5 Features    : [4, 13, 36, 48, 55, 153, 172, 174, 175, 188, 211, 212, 215, 256, 285, 301, 308, 312, 323, 355, 357, 405, 415, 423, 430, 432, 470, 474, 490, 497, 527, 534, 559, 582, 590, 597, 608, 612, 621, 639, 647, 669, 677, 686, 687, 689, 714, 798, 803, 835, 850, 856, 862, 865, 872, 890, 896, 940, 979, 1006, 1011, 1040, 1057, 1058, 1100, 1113, 1122, 1137, 1146, 1173, 1223, 1241, 1265, 1275, 1279, 1281, 1285, 1297, 1320, 1322, 1347, 1354, 1362, 1366, 1368, 1373, 1384, 1389, 1391, 1399, 1450, 1453, 1477, 1489, 1496, 1501, 1509, 1517, 1543, 1545, 1550, 1562, 1608, 1615, 1633, 1659, 1690, 1723, 1775, 1782, 1788, 1789, 1794, 1802, 1813, 1842, 1848, 1861, 1862, 1863, 1867, 1894, 1896, 1908, 1915, 1938, 1950, 2022, 2034, 2035, 2036, 2043]
  Negative Sign Features : [4, 13, 36, 48, 55, 153, 172, 174, 175, 188, 211, 212, 215, 256, 285, 301, 308, 312, 323, 355, 357, 405, 415, 423, 432, 470, 474, 490, 497, 527, 534, 559, 582, 590, 597, 608, 612, 616, 621, 639, 647, 669, 677, 686, 687, 689, 714, 798, 803, 835, 850, 856, 862, 865, 872, 890, 896, 940, 979, 1006, 1011, 1040, 1057, 1058, 1100, 1113, 1122, 1137, 1146, 1173, 1223, 1265, 1275, 1279, 1281, 1285, 1297, 1320, 1322, 1323, 1347, 1354, 1362, 1366, 1368, 1373, 1384, 1389, 1391, 1399, 1450, 1453, 1477, 1489, 1496, 1501, 1509, 1517, 1543, 1545, 1550, 1562, 1608, 1615, 1633, 1648, 1659, 1690, 1723, 1725, 1775, 1782, 1788, 1789, 1794, 1802, 1813, 1842, 1848, 1861, 1862, 1863, 1867, 1894, 1896, 1908, 1915, 1938, 1950, 2022, 2034, 2035, 2036, 2043]
  Subset 5-10 Features   : [4, 13, 36, 48, 55, 153, 172, 173, 174, 175, 188, 200, 211, 212, 215, 256, 285, 301, 308, 312, 323, 355, 357, 405, 415, 423, 430, 432, 470, 474, 490, 497, 527, 534, 559, 582, 590, 597, 608, 612, 616, 621, 639, 647, 669, 677, 686, 687, 689, 714, 798, 803, 835, 850, 856, 862, 865, 872, 890, 896, 940, 979, 984, 1006, 1011, 1040, 1057, 1058, 1100, 1113, 1122, 1137, 1146, 1173, 1223, 1241, 1265, 1275, 1279, 1281, 1285, 1297, 1320, 1322, 1323, 1347, 1354, 1362, 1366, 1368, 1373, 1384, 1389, 1391, 1399, 1414, 1450, 1453, 1477, 1489, 1496, 1501, 1509, 1517, 1543, 1545, 1550, 1562, 1608, 1615, 1633, 1648, 1659, 1690, 1723, 1725, 1734, 1775, 1782, 1788, 1789, 1794, 1802, 1813, 1842, 1848, 1861, 1862, 1863, 1867, 1894, 1896, 1908, 1915, 1938, 1950, 2022, 2034, 2035, 2036, 2043]

  DISTINCT (Non-Common) Features:
    → Subset 5-10        : [13, 173, 200, 430, 616, 984, 1241, 1323, 1414, 1648, 1725, 1734, 1908]
    → Positive Sign      : [173, 200, 430, 984, 1241, 1414, 1734]
    → Subset 0-5         : [13, 430, 1241, 1908]
    → Negative Sign      : [13, 616, 1323, 1648, 1725, 1908]

  [OK] Successfully saved 14 feature groups
=====================================================================================
</div></code></pre>
  <h5 id="surgical-ablation">Surgical Ablation</h5>
  <ul>
    <li>Since, We know the refined feature list now, let us try to perform surgical ablation to understand and quantify
      the impact in the outputs.</li>
  </ul>
  <pre class="hljs"><code><div>=====================================================================================

Kill Neg Sign          :  -9.728 (baseline) [          |█&gt;        ]  -9.336 (finalize) (+0.392)
Kill (-10, -5) Subset  :  -9.728 (baseline) [          |██&gt;       ]  -9.260 (finalize) (+0.468)
Kill Pos Sign          :   9.106 (finalize) [       &lt;██|          ]   9.691 (baseline) (-0.585)
Kill (5, 10) Subset    :   8.939 (finalize) [      &lt;███|          ]   9.691 (baseline) (-0.752)
Kill Neg Sign          :  -7.592 (baseline) [          |█&gt;        ]  -7.353 (finalize) (+0.239)
Kill (-10, -5) Subset  :  -7.592 (baseline) [          |██&gt;       ]  -7.081 (finalize) (+0.511)
Kill Neg Sign          :  -5.803 (baseline) [          |&gt;         ]  -5.759 (finalize) (+0.044)
Kill (-10, -5) Subset  :  -5.817 (finalize) [         &lt;|          ]  -5.803 (baseline) (-0.014)
Kill Neg Sign          :  -2.947 (finalize) [         &lt;|          ]  -2.918 (baseline) (-0.029)
Kill (-5, 0) Subset    :  -2.918 (baseline) [          |&gt;         ]  -2.870 (finalize) (+0.047)
Kill Neg Sign          :  -1.079 (baseline) [          |&gt;         ]  -1.047 (finalize) (+0.032)
Kill (-5, 0) Subset    :  -1.212 (finalize) [         &lt;|          ]  -1.079 (baseline) (-0.133)
Kill Pos Sign          :   0.972 (baseline) [          |&gt;         ]   1.050 (finalize) (+0.078)
Kill (0, 5) Subset     :   0.972 (baseline) [          |&gt;         ]   1.152 (finalize) (+0.179)
Kill Pos Sign          :   2.627 (finalize) [        &lt;█|          ]   2.955 (baseline) (-0.327)
Kill (0, 5) Subset     :   2.792 (finalize) [         &lt;|          ]   2.955 (baseline) (-0.162)
Kill Pos Sign          :   5.344 (finalize) [     &lt;████|          ]   6.172 (baseline) (-0.829)
Kill (5, 10) Subset    :   5.344 (finalize) [     &lt;████|          ]   6.172 (baseline) (-0.829)
Kill Pos Sign          :   7.126 (finalize) [     &lt;████|          ]   8.073 (baseline) (-0.946)
Kill (5, 10) Subset    :   7.114 (finalize) [     &lt;████|          ]   8.073 (baseline) (-0.959)

=====================================================================================
</div></code></pre>
  <ul>
    <li>we can clearly see a pattern between the concept groups &amp; Causal Shifts, as elaborated below.</li>
  </ul>
  <h6 id="high-specificity-the-%22smoking-gun%22">High Specificity (The &quot;Smoking Gun&quot;)</h6>
  <p>The most impressive part of these results is how closely the <strong>Subset</strong> ablation matches the
    <strong>Total Sign</strong> ablation.
  </p>
  <ul>
    <li><strong>Look at the last two lines:</strong> Killing the &quot;Pos Sign&quot; dropped the value by
      <strong>-0.946</strong>, while killing just the &quot;(5, 10) Subset&quot; dropped it by <strong>-0.959</strong>.
    </li>
    <li><strong>Translation:</strong> The identification of the specific &quot;circuit&quot; or neurons responsible for
      that magnitude is now clear. Because the subset ablation yields results nearly identical to the full sign
      ablation, it indicates that this is not merely the suppression of random noise; rather, the specific home of that
      numerical range has been localized.</li>
  </ul>
  <h6 id="consistency-across-magnitudes">Consistency Across Magnitudes</h6>
  <p>The results show a logical scaling.</p>
  <ul>
    <li>When the baseline values are large (e.g., <strong>9.691</strong> or <strong>8.073</strong>), the
      &quot;Kill&quot; effect is large (<strong>-0.752</strong> to <strong>-0.959</strong>).</li>
    <li>When the baseline values are small (e.g., <strong>-1.079</strong>), the &quot;Kill&quot; effect is small
      (<strong>+0.032</strong>).</li>
    <li>This suggests the ablation method is sensitive to the <strong>saliency</strong> of the features—it's not just
      breaking the model blindly.</li>
  </ul>
  <h6 id="clear-directionality">Clear Directionality</h6>
  <p>The signs are behaving exactly as expected for a successful ablation:</p>
  <ul>
    <li><strong>Ablating Negative components</strong> generally moves the value &quot;up&quot; (closer to zero/positive,
      e.g., <strong>+0.392</strong>, <strong>+0.511</strong>).</li>
    <li><strong>Ablating Positive components</strong> generally moves the value &quot;down&quot; (closer to
      zero/negative, e.g., <strong>-0.585</strong>, <strong>-0.959</strong>).</li>
  </ul>
  <h5 id="compositional-activation-steering">Compositional Activation Steering</h5>
  <ul>
    <li>We will try to shift the predicted outputs, through Activation steering aimed towards each group, so that we can
      understand the directional forces they introduce in the system.</li>
  </ul>
  <pre class="hljs"><code><div>[Calibration] Subset steering scaled by 2.978 to match sign effect.
Actual Input: [0, 7, 10, 6, 0, 1, 2, 10, 9, 2], Expected Output: -10
(Negative, Subset 5-10)

=====================================================================================
 TARGET:    -10     | INPUT LOGIC: Negative, Subset 5-10
=====================================================================================
 Original Prediction :  -9.769  [          ●         |                   ]
-------------------------------------------------------------------------------------
 Flipped: POS + SML  : -11.780  [        ●           |                   ] (Shift:  -2.01 ←)
 Steer to Positive   :  -3.961  [                ●   |                   ] (Shift:  +5.81 →)
 Flipped: POS + LRG  :  -0.456  [                   ●|                   ] (Shift:  +9.31 →)
 Steer to Subset 5-10:  -3.307  [                ●   |                   ] (Shift:  +6.46 →)
 Flipped: NEG + LRG  :  -7.762  [            ●       |                   ] (Shift:  +2.01 →)
 Steer to Negative   : -15.798  [    ●               |                   ] (Shift:  -6.03 ←)
 Flipped: NEG + SML  : -30.627  [●                   |                   ] (Shift: -20.86 ←)
 Steer to Subset 0-5 : -21.927  [●                   |                   ] (Shift: -12.16 ←)
-------------------------------------------------------------------------------------
Actual Input: [1, 7, 9, 10, 3, 1, 10, 5, 0, 3], Expected Output: 10
(Positive, Subset 5-10)

=====================================================================================
 TARGET:     10     | INPUT LOGIC: Positive, Subset 5-10
=====================================================================================
 Original Prediction :   9.691  [                    |        ●          ]
-------------------------------------------------------------------------------------
 Flipped: POS + SML  :  10.302  [                    |         ●         ] (Shift:  +0.61 →)
 Steer to Positive   :  13.716  [                    |            ●      ] (Shift:  +4.03 →)
 Flipped: POS + LRG  :  11.708  [                    |          ●        ] (Shift:  +2.02 →)
 Steer to Subset 5-10:   7.693  [                    |      ●            ] (Shift:  -2.00 ←)
 Flipped: NEG + LRG  :   3.057  [                    |  ●                ] (Shift:  -6.63 ←)
 Steer to Negative   :   5.239  [                    |    ●              ] (Shift:  -4.45 ←)
 Flipped: NEG + SML  :  -6.408  [             ●      |                   ] (Shift: -16.10 ←)
 Steer to Subset 0-5 :   2.466  [                    | ●                 ] (Shift:  -7.22 ←)
-------------------------------------------------------------------------------------
Actual Input: [9, 10, 8, 1, 3, 10, 1, 0, 9, 3], Expected Output: -8
(Negative, Subset 5-10)

=====================================================================================
 TARGET:     -8     | INPUT LOGIC: Negative, Subset 5-10
=====================================================================================
 Original Prediction :  -7.428  [            ●       |                   ]
-------------------------------------------------------------------------------------
 Flipped: POS + SML  :  -8.287  [           ●        |                   ] (Shift:  -0.86 ←)
 Steer to Positive   :  -3.946  [                ●   |                   ] (Shift:  +3.48 →)
 Flipped: POS + LRG  :  -2.917  [                 ●  |                   ] (Shift:  +4.51 →)
 Steer to Subset 5-10:  -5.682  [              ●     |                   ] (Shift:  +1.75 →)
 Flipped: NEG + LRG  :  -8.730  [           ●        |                   ] (Shift:  -1.30 ←)
 Steer to Negative   : -11.192  [        ●           |                   ] (Shift:  -3.76 ←)
 Flipped: NEG + SML  : -20.768  [●                   |                   ] (Shift: -13.34 ←)
 Steer to Subset 0-5 : -15.463  [    ●               |                   ] (Shift:  -8.04 ←)
-------------------------------------------------------------------------------------
Actual Input: [9, 0, 1, 8, 2, 3, 9, 7, 5, 2], Expected Output: -6
(Negative, Subset 5-10)

=====================================================================================
 TARGET:     -6     | INPUT LOGIC: Negative, Subset 5-10
=====================================================================================
 Original Prediction :  -5.803  [              ●     |                   ]
-------------------------------------------------------------------------------------
 Flipped: POS + SML  :  -6.120  [             ●      |                   ] (Shift:  -0.32 ←)
 Steer to Positive   :   2.667  [                    | ●                 ] (Shift:  +8.47 →)
 Flipped: POS + LRG  :   1.306  [                    |●                  ] (Shift:  +7.11 →)
 Steer to Subset 5-10:  -3.104  [                ●   |                   ] (Shift:  +2.70 →)
 Flipped: NEG + LRG  :  -7.221  [            ●       |                   ] (Shift:  -1.42 ←)
 Steer to Negative   : -14.065  [     ●              |                   ] (Shift:  -8.26 ←)
 Flipped: NEG + SML  : -39.573  [●                   |                   ] (Shift: -33.77 ←)
 Steer to Subset 0-5 : -22.613  [●                   |                   ] (Shift: -16.81 ←)
-------------------------------------------------------------------------------------
Actual Input: [7, 3, 6, 5, 3, 3, 2, 8, 8, 2], Expected Output: -3
(Negative, Subset 0-5)

=====================================================================================
 TARGET:     -3     | INPUT LOGIC: Negative, Subset 0-5
=====================================================================================
 Original Prediction :  -2.936  [                 ●  |                   ]
-------------------------------------------------------------------------------------
 Flipped: POS + SML  :  -1.457  [                  ● |                   ] (Shift:  +1.48 →)
 Steer to Positive   :   4.377  [                    |   ●               ] (Shift:  +7.31 →)
 Flipped: POS + LRG  :   1.502  [                    |●                  ] (Shift:  +4.44 →)
 Steer to Subset 5-10:  -1.130  [                  ● |                   ] (Shift:  +1.81 →)
 Flipped: NEG + LRG  :  -4.739  [               ●    |                   ] (Shift:  -1.80 ←)
 Steer to Negative   :  -9.510  [          ●         |                   ] (Shift:  -6.57 ←)
 Flipped: NEG + SML  : -28.173  [●                   |                   ] (Shift: -25.24 ←)
 Steer to Subset 0-5 : -14.208  [     ●              |                   ] (Shift: -11.27 ←)
-------------------------------------------------------------------------------------
Actual Input: [1, 6, 4, 6, 1, 3, 9, 3, 7, 3], Expected Output: -1
(Negative, Subset 0-5)

=====================================================================================
 TARGET:     -1     | INPUT LOGIC: Negative, Subset 0-5
=====================================================================================
 Original Prediction :  -1.066  [                  ● |                   ]
-------------------------------------------------------------------------------------
 Flipped: POS + SML  :   1.296  [                    |●                  ] (Shift:  +2.36 →)
 Steer to Positive   :   5.111  [                    |    ●              ] (Shift:  +6.18 →)
 Flipped: POS + LRG  :   4.125  [                    |   ●               ] (Shift:  +5.19 →)
 Steer to Subset 5-10:  -0.989  [                   ●|                   ] (Shift:  +0.08 →)
 Flipped: NEG + LRG  :  -4.817  [               ●    |                   ] (Shift:  -3.75 ←)
 Steer to Negative   :  -7.073  [            ●       |                   ] (Shift:  -6.01 ←)
 Flipped: NEG + SML  : -25.059  [●                   |                   ] (Shift: -23.99 ←)
 Steer to Subset 0-5 : -13.298  [      ●             |                   ] (Shift: -12.23 ←)
-------------------------------------------------------------------------------------
Actual Input: [5, 1, 4, 4, 3, 5, 9, 3, 3, 2], Expected Output: 1
(Positive, Subset 0-5)

=====================================================================================
 TARGET:     1      | INPUT LOGIC: Positive, Subset 0-5
=====================================================================================
 Original Prediction :   0.999  [                    ●                   ]
-------------------------------------------------------------------------------------
 Flipped: POS + SML  :  -0.288  [                   ●|                   ] (Shift:  -1.29 ←)
 Steer to Positive   :   6.477  [                    |     ●             ] (Shift:  +5.48 →)
 Flipped: POS + LRG  :   2.020  [                    | ●                 ] (Shift:  +1.02 →)
 Steer to Subset 5-10:   0.096  [                    ●                   ] (Shift:  -0.90 ←)
 Flipped: NEG + LRG  :  -2.206  [                 ●  |                   ] (Shift:  -3.21 ←)
 Steer to Negative   :  -6.798  [             ●      |                   ] (Shift:  -7.80 ←)
 Flipped: NEG + SML  : -30.240  [●                   |                   ] (Shift: -31.24 ←)
 Steer to Subset 0-5 : -15.376  [    ●               |                   ] (Shift: -16.38 ←)
-------------------------------------------------------------------------------------
Actual Input: [1, 6, 10, 5, 2, 5, 5, 3, 7, 3], Expected Output: 3
(Positive, Subset 0-5)

=====================================================================================
 TARGET:     3      | INPUT LOGIC: Positive, Subset 0-5
=====================================================================================
 Original Prediction :   2.963  [                    | ●                 ]
-------------------------------------------------------------------------------------
 Flipped: POS + SML  :  11.193  [                    |          ●        ] (Shift:  +8.23 →)
 Steer to Positive   :   9.265  [                    |        ●          ] (Shift:  +6.30 →)
 Flipped: POS + LRG  :   6.921  [                    |     ●             ] (Shift:  +3.96 →)
 Steer to Subset 5-10:   0.790  [                    ●                   ] (Shift:  -2.17 ←)
 Flipped: NEG + LRG  :  -2.957  [                 ●  |                   ] (Shift:  -5.92 ←)
 Steer to Negative   :  -3.511  [                ●   |                   ] (Shift:  -6.47 ←)
 Flipped: NEG + SML  : -19.539  [●                   |                   ] (Shift: -22.50 ←)
 Steer to Subset 0-5 :  -4.637  [               ●    |                   ] (Shift:  -7.60 ←)
-------------------------------------------------------------------------------------
Actual Input: [5, 4, 8, 8, 2, 5, 3, 1, 2, 3], Expected Output: 6
(Positive, Subset 5-10)

=====================================================================================
 TARGET:     6      | INPUT LOGIC: Positive, Subset 5-10
=====================================================================================
 Original Prediction :   6.170  [                    |     ●             ]
-------------------------------------------------------------------------------------
 Flipped: POS + SML  :  11.856  [                    |          ●        ] (Shift:  +5.69 →)
 Steer to Positive   :  11.122  [                    |          ●        ] (Shift:  +4.95 →)
 Flipped: POS + LRG  :   9.794  [                    |        ●          ] (Shift:  +3.62 →)
 Steer to Subset 5-10:   4.336  [                    |   ●               ] (Shift:  -1.83 ←)
 Flipped: NEG + LRG  :  -0.814  [                   ●|                   ] (Shift:  -6.98 ←)
 Steer to Negative   :  -0.880  [                   ●|                   ] (Shift:  -7.05 ←)
 Flipped: NEG + SML  : -15.654  [    ●               |                   ] (Shift: -21.82 ←)
 Steer to Subset 0-5 :  -0.469  [                   ●|                   ] (Shift:  -6.64 ←)
-------------------------------------------------------------------------------------
Actual Input: [10, 9, 7, 7, 1, 6, 7, 10, 1, 3], Expected Output: 8
(Positive, Subset 5-10)

=====================================================================================
 TARGET:     8      | INPUT LOGIC: Positive, Subset 5-10
=====================================================================================
 Original Prediction :   7.999  [                    |      ●            ]
-------------------------------------------------------------------------------------
 Flipped: POS + SML  :   8.607  [                    |       ●           ] (Shift:  +0.61 →)
 Steer to Positive   :  12.472  [                    |           ●       ] (Shift:  +4.47 →)
 Flipped: POS + LRG  :  11.249  [                    |          ●        ] (Shift:  +3.25 →)
 Steer to Subset 5-10:   7.463  [                    |      ●            ] (Shift:  -0.54 ←)
 Flipped: NEG + LRG  :   2.692  [                    | ●                 ] (Shift:  -5.31 ←)
 Steer to Negative   :   2.352  [                    | ●                 ] (Shift:  -5.65 ←)
 Flipped: NEG + SML  : -13.023  [      ●             |                   ] (Shift: -21.02 ←)
 Steer to Subset 0-5 :  -0.244  [                   ●|                   ] (Shift:  -8.24 ←)
-------------------------------------------------------------------------------------
[OK] Feature analysis complete
</div></code></pre>
  <ul>
    <li>We can see broadly that the prediction output co-relates with the Expectation closely, thereby latent steering
      introducing quadratic shifts on the value, based on the orthogonal property of the latent vectors.</li>
  </ul>
  <h6 id="the-%22steer-to-subset%22-logic-is-working">The &quot;Steer to Subset&quot; Logic is Working</h6>
  <p>The most important test for any subset steering is: <em>Does steering to a specific magnitude range move the
      prediction toward that range?</em></p>
  <ul>
    <li><strong>Example (Target -10):</strong> When we &quot;Steer to Negative,&quot; the value drops to
      <strong>-15.798</strong>.
    </li>
    <li><strong>Example (Target -1):</strong> When we &quot;Steer to Subset 0-5&quot; (Small Negative), the shift is a
      massive <strong>-12.23</strong>, dragging the prediction deep into the negative territory.</li>
    <li><strong>The Contrast:</strong> Notice how &quot;Steer to Subset 5-10&quot; has a much smaller effect on the
      low-value inputs than the high-value ones. This shows our calibration (the 2.978 scaling) is working to
      differentiate between &quot;just being negative&quot; and &quot;being a specific kind of negative.&quot;</li>
  </ul>
  <h6 id="successful-%22flipping%22-the-counterfactual-test">Successful &quot;Flipping&quot; (The Counterfactual Test)
  </h6>
  <p>A &quot;good&quot; result in mechanistic interpretability is often defined by whether one can make the model
    &quot;hallucinate&quot; the opposite sign.</p>
  <ul>
    <li><strong>Target -6 (Negative):</strong> By &quot;Steering to Positive,&quot; we moved the prediction from
      <strong>-5.803</strong> to <strong>+2.667</strong>.
    </li>
    <li><strong>Target 1 (Positive):</strong> By &quot;Steering to Negative,&quot; we moved it from
      <strong>+0.999</strong> to <strong>-6.798</strong>.
    </li>
    <li><strong>Significance:</strong> We aren't just nudging the model; we are successfully crossing the
      zero-threshold. This confirms those features are the primary drivers of the sign bit.</li>
  </ul>
  <h6 id="the-%22neg--sml%22-negative--small-explosion">The &quot;NEG + SML&quot; (Negative + Small) Explosion</h6>
  <p>One might notice that &quot;Flipped: NEG + SML&quot; often results in huge shifts (e.g., <strong>-39.573</strong>
    or <strong>-30.627</strong>).</p>
  <ul>
    <li><strong>Why this is &quot;Good&quot;:</strong> This usually suggests that the &quot;Small&quot; and
      &quot;Negative&quot; features are highly concentrated or have high activation density. When we amplify them
      (&quot;flip&quot; them), it is likely saturating the model's output. It shows these features are incredibly
      &quot;potent&quot; within the model's latent space.</li>
  </ul>
  <h6 id="meaningful-sub-distinctions">Meaningful Sub-Distinctions</h6>
  <p>Look at the <strong>Target 8</strong> (Positive, Subset 5-10) block:</p>
  <ul>
    <li><strong>Steer to Negative:</strong> Prediction goes to <strong>2.352</strong>.</li>
    <li><strong>Steer to Subset 0-5:</strong> Prediction goes to <strong>-0.244</strong>.</li>
    <li><strong>Observation:</strong> Steering to the &quot;0-5&quot; (Small) subset actually pushed the model further
      toward the negative/zero than a general &quot;Negative&quot; steer did. This suggests our &quot;Subset 0-5&quot;
      feature set is capturing a very specific &quot;reduction&quot; logic that the model uses to keep values low.</li>
  </ul>
  <h5 id="alpha-sweep-compliance-against-ood-datasets">Alpha Sweep Compliance against OOD Datasets</h5>
  <ul>
    <li>We Tried Normalized Activation steering Against OOD dataset, representing different attributes, like
      interpolation, extrapolation, scaling &amp; precision.</li>
  </ul>
  <pre class="hljs"><code><div>======================================================================
 STEERING VALIDATION &amp; COMPLIANCE TESTING
======================================================================

  -&gt; Calibrating feature scales using dataset/interp_test.xlsx...
  [OK] Calibration: Sign_std=357.0766, Subset_std=149.7860
  [OK] Subset steering scaled by 1.033 to match sign effect.

  1. Testing Interpolation (In-Distribution)...
  → Validating 1000 samples from interp_test...

======================================================================
  STEERING SUCCESS RATES (Alpha = 2.00)
======================================================================
  [OK] Sign Flip Success   :  25.00%
  [OK] Subset Flip Success :  52.10%
  [OK] Full Quadrant Flip  :  10.50%
======================================================================

  2. Testing Extrapolation (Out-of-Distribution)...
  → Validating 1000 samples from extrap_test...

======================================================================
  STEERING SUCCESS RATES (Alpha = 2.00)
======================================================================
  [OK] Sign Flip Success   :  35.60%
  [OK] Subset Flip Success :  91.50%
  [OK] Full Quadrant Flip  :  35.00%
======================================================================

  3. Testing Scaling (Magnitude Shift)...
  → Validating 1000 samples from scaling_test...

======================================================================
  STEERING SUCCESS RATES (Alpha = 2.00)
======================================================================
  [OK] Sign Flip Success   :  75.00%
  [OK] Subset Flip Success : 100.00%
  [OK] Full Quadrant Flip  :  75.00%
======================================================================

  4. Testing Precision (Float Values)...
  → Validating 1000 samples from precision_test...

======================================================================
  STEERING SUCCESS RATES (Alpha = 2.00)
======================================================================
  [OK] Sign Flip Success   :  24.60%
  [OK] Subset Flip Success :  48.30%
  [OK] Full Quadrant Flip  :  11.40%
======================================================================
</div></code></pre>
  <h6 id="the-%22extrapolation%22-paradox-the-biggest-win">The &quot;Extrapolation&quot; Paradox (The Biggest Win)</h6>
  <p>Usually, models break when they see Out-of-Distribution (OOD) data. Our results show the opposite:</p>
  <ul>
    <li><strong>Subset Flip Success (91.50%)</strong> and <strong>Sign Flip Success (35.60%)</strong> are much higher in
      Extrapolation than in Interpolation (52.1% and 25%).</li>
    <li><strong>Interpretation:</strong> This suggests that as values get larger or move outside the training range, the
      model relies <strong>more</strong> on these specific &quot;Sign&quot; and &quot;Subset&quot; directions and less
      on specific memorized quirks of the data. Our features are &quot;cleaner&quot; in the extrapolation regime.</li>
  </ul>
  <h6 id="scaling-test-perfect-control">Scaling Test (Perfect Control)</h6>
  <ul>
    <li><strong>Subset Flip Success: 100.00%</strong></li>
    <li><strong>Sign Flip Success: 75.00%</strong></li>
    <li><strong>Interpretation:</strong> This is the &quot;gold standard.&quot; When the model is dealing with simple
      scaling, our steering vectors have total authority. A 100% success rate on subset flipping is rare in mechanistic
      interpretability—it means we have perfectly isolated the Subset neurons.</li>
  </ul>
  <h6 id="precision-vs-interpolation-the-%22hard%22-cases">Precision vs. Interpolation (The &quot;Hard&quot; Cases)</h6>
  <p>The lower success rates in <strong>Interpolation (25%)</strong> and <strong>Precision (24.6%)</strong> for sign
    flipping tell us two things:</p>
  <ul>
    <li><strong>Feature Density:</strong> In the &quot;normal&quot; range or with precise floats, the model likely has
      many overlapping features (polysemanticity). A single steering vector at alpha = 2.00 isn't enough to overcome
      the natural &quot;gravity&quot; of the actual input.</li>
    <li><strong>The &quot;Stickiness&quot; of Sign:</strong> The model seems much more &quot;stubborn&quot; about the
      positive/negative sign than it is about the subset. It is easier to make a &quot;Large&quot; number
      &quot;Small&quot; than it is to make a &quot;Negative&quot; number &quot;Positive.&quot;</li>
  </ul>
  <h6 id="summary-table-success-hierarchy">Summary Table: Success Hierarchy</h6>
  <table>
    <thead>
      <tr>
        <th>Test Category</th>
        <th>Ease of Steering</th>
        <th>Key Insight</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Scaling</strong></td>
        <td>🌟 Highest</td>
        <td>Purest representation of the features.</td>
      </tr>
      <tr>
        <td><strong>Extrapolation</strong></td>
        <td>✅ High</td>
        <td>Features generalize better than they interpolate.</td>
      </tr>
      <tr>
        <td><strong>Interpolation</strong></td>
        <td>⚠️ Moderate</td>
        <td>Internal &quot;noise&quot; or data-specific logic resists steering.</td>
      </tr>
      <tr>
        <td><strong>Precision</strong></td>
        <td>⚠️ Moderate</td>
        <td>Floating point nuances slightly degrade vector alignment.</td>
      </tr>
    </tbody>
  </table>
  <h5 id="extended-alpha-sweep-compliance-against-ood-datasets">Extended Alpha Sweep Compliance against OOD Datasets
  </h5>
  <ul>
    <li>We have extended the alpha to 1024, to observe patterns, if any.</li>
  </ul>
  <p><img src="images/unified_logic_heatmap.png" alt="alpha-sweep-heatmap">
    <em>Figure 3: Heatmap shows OOD Dataset vs Alpha Sweep Compliance, interms of percentage that defines a
      transition.</em>
  </p>
  <h6 id="the-%22sign%22-vs-%22subset%22-trade-off">The &quot;Sign&quot; vs. &quot;Subset&quot; Trade-off</h6>
  <p>There is a clear <strong>inverse relationship</strong> between sign accuracy and subset accuracy as alpha gets
    increased:</p>
  <ul>
    <li><strong>Sign Accuracy</strong> starts low and <strong>explodes</strong> at very high Alphas (512.0+).</li>
    <li><strong>Subset Accuracy</strong> starts high and <strong>collapses</strong> at high Alphas (512.0+).</li>
  </ul>
  <p><strong>The Verdict:</strong> Our &quot;Sign&quot; feature is much &quot;stiffer&quot; or more deeply embedded than
    our &quot;Subset&quot; feature. To force the model to flip a sign, we have to apply a massive amount of pressure
    (Alpha &gt; 128), but doing so essentially <strong>obliterates</strong> the model's ability to maintain magnitude
    precision (the Subset Accuracy drops from ~52% to ~27%).</p>
  <h6 id="the-%22scaling%22-anchor">The &quot;Scaling&quot; Anchor</h6>
  <p>The <strong>Scaling</strong> dataset is our most robust result. It stays flat at <strong>75% (Sign)</strong> and
    <strong>100% (Subset)</strong> across the entire sweep.
  </p>
  <ul>
    <li><strong>Why this is good:</strong> This proves that for the simplest version of this task, our steering vectors
      are perfectly aligned. The fact that it doesn't degrade even at Alpha 1024.0 means the &quot;Scaling&quot; logic
      is perfectly linear and isolated in the model's latent space.</li>
  </ul>
  <h6 id="extrapolation-is-the-%22goldilocks%22-zone">Extrapolation is the &quot;Goldilocks&quot; Zone</h6>
  <p>Extrapolation performs significantly better than Interpolation until the very end of the sweep.</p>
  <ul>
    <li>At Alpha 256.0, Extrapolation hit <strong>55.4% Sign / 90.7% Subset</strong>.</li>
    <li>At the same Alpha, Interpolation was only at <strong>38.9% Sign / 48.6% Subset</strong>.</li>
    <li><strong>Insight:</strong> This suggests the model's representations of &quot;extreme&quot; values are much
      cleaner and more &quot;mathematically pure&quot; than its representations of common, in-distribution values, which
      are likely cluttered with specialized logic or memorization.</li>
  </ul>
  <h6 id="key-observations-by-dataset">Key Observations by Dataset</h6>
  <table>
    <thead>
      <tr>
        <th>Dataset</th>
        <th>Peak Total Acc</th>
        <th>Behavior</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Interpolation</strong></td>
        <td><strong>27.5%</strong> (@1024)</td>
        <td>Very resistant. Requires &quot;brute force&quot; steering to see any sign change.</td>
      </tr>
      <tr>
        <td><strong>Extrapolation</strong></td>
        <td><strong>53.6%</strong> (@512)</td>
        <td>Highly steerable. This is where our features are most &quot;causal.&quot;</td>
      </tr>
      <tr>
        <td><strong>Scaling</strong></td>
        <td><strong>75.0%</strong> (Flat)</td>
        <td>Pure linear behavior. We've perfectly captured this circuit.</td>
      </tr>
      <tr>
        <td><strong>Precision</strong></td>
        <td><strong>29.3%</strong> (@1024)</td>
        <td>Like Interpolation, shows that &quot;fine-grained&quot; data resists broad steering vectors.</td>
      </tr>
    </tbody>
  </table>
  <h4 id="generating-reports">Generating Reports</h4>
  <pre class="hljs"><code><div>[8/8] &gt; Generating Feature Reports...

======================================================================
 GENERATING VISUALIZATION SUITE
======================================================================

  -&gt; Generating Steering Basis Compass...
     Successfully generated concept compass with zoomed-in &amp; zoomed out views.
  -&gt; Generating Performance Heatmaps &amp; Pareto Frontier...
     Successfully generated unified heatmap and Pareto frontier in /images.
  -&gt; Generating Logit-Lens Visualizations...
     Success: Unified Logit-Lens generated for 141 features.

======================================================================
  [OK] VISUALIZATION SUITE COMPLETE
  All visualizations exported to images/ folder
======================================================================

[OK] Reports generated successfully
</div></code></pre>
  <h3 id="experiment-inference">Experiment Inference</h3>
  <h4 id="mechanistic-interpretability-of-mlp-circuits">Mechanistic Interpretability of MLP Circuits</h4>
  <ul>
    <li>This experiment successfully executed a full end-to-end pipeline to deconstruct the internal logic of a
      Multi-Layer Perceptron (MLP) using a <strong>Top-K Sparse Autoencoder (SAE)</strong>. By forcing the model’s
      internal representations through a bottleneck of discrete &quot;dictionary features,&quot; we have successfully
      mapped the &quot;black-box&quot; hidden layers into traceable, causal circuits.</li>
  </ul>
  <h4 id="model-convergence--reconstruction-fidelity">Model Convergence &amp; Reconstruction Fidelity</h4>
  <ul>
    <li><em><strong>MLP Performance</strong></em>: The MLP reached &quot;interpretable perfection&quot; for the
      Index-Based Arithmetic task. The training achieved an overall <strong>Test MSE of 0.034669</strong>, effectively
      solving the pointer-routing and subtraction logic across all concept groups (Positive/Negative, Small/Large
      Subsets).</li>
    <li><em><strong>SAE Efficiency</strong></em>: Utilizing a <strong>Top-K activation function (k=128)</strong>, the
      Sparse Autoencoder achieved a reconstruction <strong>MSE of 0.000109</strong> by Epoch 100. Because Top-K
      eliminates the &quot;shrinkage&quot; effect found in L1-based SAEs, the recovered features maintain 100% of their
      causal potency for downstream steering.</li>
  </ul>
  <h4 id="identification-of-specialist-features">Identification of Specialist Features</h4>
  <p>The Feature Probing phase identified specific SAE latents that act as &quot;Specialists&quot; for the model's
    logical quadrants. By analyzing the steering basis, we identified:</p>
  <ul>
    <li><em><strong>The &quot;Subset&quot; Controllers</strong></em>: Features associated with <strong>Subset
        0-5</strong> and <strong>Subset 5-10</strong> were identified. The pipeline proved that these features are
      disentangled from the sign; steering to &quot;Subset 5-10&quot; while maintaining a &quot;Positive&quot; sign
      successfully moved outputs across the number line with high precision.</li>
    <li><em><strong>Sparsity Constraints</strong></em>: By enforcing a hard <strong>L_0 = 128</strong>, the model
      utilizes exactly <strong>6.25%</strong> of its 2048-feature capacity per inference. This ensures each active
      feature is <strong>monosemantic</strong>, representing a single logical component of the pointer-arithmetic task.
    </li>
  </ul>
  <h4 id="structural-circuit-trace">Structural Circuit Trace</h4>
  <p>The pipeline generated a suite of visual reports to confirm the &quot;Concept Geometry&quot; of the model (refer to
    <code>images/</code> directory):
  </p>
  <ul>
    <li><em><strong>The Steering Basis Compass</strong></em>: Visualizes the geometric orientation of the Sign vs.
      Subset vectors. It confirms that &quot;Positive&quot; and &quot;Negative&quot; latents are represented as opposing
      directions in the hidden space.</li>
  </ul>
  <p><img src="images/concept_compass_elegant.png" alt="concept-compass">
    <em>Figure 4: Logic basis geometric disentanglement showing orthogonal sign and subset vectors. Cosine similarity =
      -0.02 validates perfect disentanglement.</em>
  </p>
  <ul>
    <li>The above image shows the orthogonality of two vectors that namely represents Sign &amp; Subset vectors, which
      is quantifiably verified using cosine similarity after training both MLP &amp; SAE, for confirmation before
      starting the Ablation &amp; Activation Steering experiments.</li>
  </ul>
  <ul>
    <li><em><strong>The Unified Logit-Lens</strong></em>: Provides the definitive &quot;Causal Map,&quot; showing
      exactly how <strong>141 identified features</strong> contribute to the final logit. This heatmap identifies which
      SAE features &quot;push&quot; the output toward specific arithmetic values.</li>
  </ul>
  <p><img src="images/unified_logit_lens.png" alt="logit-lens">
    <em>Figure 5: Logit Map contains the logit distribution across 5 group of layers, which are listed below.</em>
  </p>
  <ul>
    <li>
      <p>Logits are distributed to different concept groups, in the order of y - axis.</p>
      <ul>
        <li><em><strong>Primitive four layers</strong></em> : { POS + SML }, { POS + LRG }, { NEG + SML }, { NEG + LRG }
        </li>
        <li><em><strong>Derived Union Layer</strong></em> : { POS + SML } ∪ { POS + LRG } ∪ { NEG + SML } ∪ { NEG + LRG
          }</li>
        <li><em><strong>Primitive Distinct Layers</strong></em> : { POS }, { NEG }, { SML }, { LRG }</li>
        <li><em><strong>Derived Intersection Layer</strong></em> : { POS } ∩ { NEG } ∩ { SML } ∩ { LRG }</li>
        <li><em><strong>Derived Distinct Layers</strong></em> : { POS } - { DIL }, { NEG } - { DIL }, { SML } - { DIL },
          { LRG } - { DIL } ( DIL - Derived Intersection Layer )</li>
      </ul>
    </li>
  </ul>
  <ul>
    <li><em><strong>The Pareto Frontier</strong></em>: Illustrates the trade-off total between Compliance and alpha
      against the 4 distinct OOD datasets, proving where the model captures maximum logic with minimum feature
      activation.</li>
  </ul>
  <p><img src="images/unified_pareto_frontier.png" alt="pareto-frontier">
    <em>Figure 6: Better Prediction performance on Extrapolation &amp; Scaling dataset, portrays the models capability
      to generalize the logic, instead of mere memorization of the exact training dataset.</em>
  </p>
  <h3 id="conclusion">Conclusion</h3>
  <p>The experiment proves that the MLP's arithmetic logic is concentrated into traceable, steerable circuits rather
    than being scattered randomly across neurons. With a total execution time of <strong>9 minutes and 42
      seconds</strong>, the pipeline produced a high-fidelity map of the model's &quot;internal engine.&quot; The
    discovered latents are not just correlations; they are <strong>causal levers</strong> that allow for precise
    manipulation of the model's behavior.</p>
  <h3 id="future-work">Future Work</h3>
  <p>Building on our methodology for decomposing and steering latent features, we plan to extend this work in the
    following directions:</p>
  <p><strong>1. Transfer to Real Language Models</strong></p>
  <ul>
    <li>Apply SAE-based steering to Pythia-70M for style control (formality, politeness, brevity)</li>
    <li>Investigate whether disentanglement principles discovered in toy models transfer to transformers</li>
    <li>Test compositional steering: Can we simultaneously control multiple attributes (formal ∧ polite ∧ brief)?</li>
  </ul>
  <p><strong>2. Fundamental Trade-offs</strong></p>
  <ul>
    <li>Quantify the coherence-style trade-off: Does strong style steering degrade output quality?</li>
    <li>Identify optimal α selection strategies for deployment scenarios</li>
    <li>Compare trade-off patterns across model scales (Pythia 70M → 160M → 410M)</li>
  </ul>
  <p><strong>3. Mechanistic Understanding</strong></p>
  <ul>
    <li>Determine which transformer layers encode style vs. content</li>
    <li>Investigate why certain attributes are &quot;stiffer&quot; than others (analogous to sign vs. magnitude in our
      MLP)</li>
    <li>Use causal tracing to map complete style circuits in the residual stream</li>
  </ul>
  <p><strong>4. Practical Applications</strong></p>
  <ul>
    <li>Develop steering strategies that preserve coherence while achieving reliable style control</li>
    <li>Test robustness across diverse prompts and domains</li>
    <li>Evaluate whether steering can replace fine-tuning for style adaptation</li>
  </ul>
  <h3 id="references">References</h3>
  <p>If you use this codebase for research, please cite the original paper that inspired this architecture:</p>
  <p><em><strong>Bricken, T., et al. (2023). Towards Monosemanticity: Decomposing Language Models with Dictionary
        Learning. Transformer Circuits Thread.</strong></em></p>
  <p><em><strong>Turner, A., et al. (2023). Activation Addition: Steering Language Models Without Optimization.
        arXiv:2308.10248.</strong></em></p>
  <p><em><strong>Meng, K., et al. (2022). Locating and Editing Factual Associations in GPT. NeurIPS, 35,
        17359-17372.</strong></em></p>
  <p><em><strong>Cunningham, H., et al. (2023). Sparse Autoencoders Find Highly Interpretable Features in Language
        Models. arXiv:2309.08600.</strong></em></p>
  <p><em><strong>Biderman, S., et al. (2023). Pythia: A Suite for Analyzing Large Language Models Across Training and
        Scaling. ICML.</strong></em></p>

</body>

</html>